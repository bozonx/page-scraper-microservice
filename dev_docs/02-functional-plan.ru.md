# Функциональный план микросервиса Page Scraper

## Назначение микросервиса

Микросервис Page Scraper предназначен для извлечения структурированных данных из веб-страниц по переданному URL. Основная задача сервиса — получить на вход URL и параметры скрапера, а вернуть JSON с основными данными страницы и статьёй в формате Markdown.

## Архитектура микросервиса

```mermaid
graph TB
    Client[Клиент] --> API[REST API]
    API --> Validation[Валидация запроса]
    Validation --> Scraper[Модуль скрапинга]
    Scraper --> Browser[Браузерный движок]
    Scraper --> Extractor[Извлечение контента]
    Extractor --> Converter[Конвертер в Markdown]
    Converter --> Response[Формирование ответа]
    Response --> Client
    
    Config[Конфигурация] --> Scraper
    Config --> API
```

## Основные функции

### 1. Скрапинг отдельных страниц
- Извлечение контента из HTML страниц
- Поддержка двух режимов скрапинга: Cheerio (статический контент) и Playwright (динамический контент)
- Конфигурируемые параметры извлечения (таймауты, пользовательские агенты, локализация)
- Защита от антибот систем с использованием генерации отпечатков браузера

### 2. Пакетная обработка
- Обработка нескольких URL в одном запросе
- Управление задержками между запросами для имитации человеческого поведения
- Асинхронная обработка с уведомлением через вебхуки с настраиваемыми попытками повторной отправки

## API интерфейсы

### Базовый URL
`/{API_BASE_PATH}/v1`

### 1. Скрапинг отдельной страницы
**POST** `/page`

Извлекает данные с отдельной веб-страницы.

**Запрос:**
```json
{
  "url": "https://example.com/article",
  "mode": "cheerio|playwright",
  "taskTimeoutSecs": 30,
  "locale": "en-US",
  "dateLocale": "ru",
  "timezoneId": "UTC",
  "blockTrackers": true,
  "blockHeavyResources": true,
  "fingerprint": {
    "generate": true,
    "userAgent": "auto",
    "locale": "source",
    "timezoneId": "source",
    "rotateOnAntiBot": true,
    "generator": {
      "browsers": ["chrome"]
    }
  }
}
```

**Ответ:**
```json
{
  "url": "https://example.com/article",
  "title": "Article title",
  "description": "Optional description",
  "date": "2024-11-12T10:00:00.000Z",
  "author": "John Doe",
  "body": "# Article title\n\nMarkdown content ...",
  "meta": {
    "lang": "en",
    "readTimeMin": 5
  }
}
```

### 2. Пакетная обработка
**POST** `/batch`

Обрабатывает несколько страниц с задержками и уведомлением по вебхуку.

**Запрос:**
```json
{
  "items": [
    { "url": "https://site1.com/a1", "mode": "playwright" },
    { "url": "https://site2.com/a2" }
  ],
  "commonSettings": {
    "mode": "cheerio",
    "taskTimeoutSecs": 30,
    "locale": "en-US",
    "timezoneId": "UTC",
    "dateLocale": "ru",
    "blockTrackers": true,
    "blockHeavyResources": true,
    "fingerprint": {
      "generate": true,
      "userAgent": "auto",
      "locale": "source",
      "timezoneId": "source",
      "rotateOnAntiBot": true,
      "generator": {
        "browsers": ["chrome"]
      }
    }
  },
  "schedule": {
    "minDelayMs": 1500,
    "maxDelayMs": 4000,
    "jitter": true,
    "concurrency": 1
  },
  "webhook": {
    "url": "https://example.com/webhook",
    "headers": { "X-Source": "page-scraper" },
    "backoffMs": 1000,
    "maxAttempts": 3
  }
}
```

**Ответ:**
```json
{ "jobId": "b-20241112-abcdef" }
```


### 3. Проверка состояния
**GET** `/health`

Проверяет работоспособность микросервиса.

## Переменные окружения

### Основные настройки
- `NODE_ENV` — режим работы (`production|development|test`)
- `LISTEN_HOST` — хост для прослушивания (`0.0.0.0` или `localhost`)
- `LISTEN_PORT` — порт для прослушивания (по умолчанию `80`)
- `API_BASE_PATH` — префикс API (по умолчанию `api`)
- `LOG_LEVEL` — уровень логирования (`trace|debug|info|warn|error|fatal|silent`)
- `TZ` — часовой пояс (по умолчанию `UTC`)

### Настройки скрапера
- `DEFAULT_MODE` — режим скрапинга по умолчанию (`cheerio|playwright`)
- `DEFAULT_TASK_TIMEOUT_SECS` — таймаут задачи по умолчанию в секундах (по умолчанию `30`)
- `DEFAULT_USER_AGENT` — пользовательский агент по умолчанию (`auto|desktop|mobile`)
- `DEFAULT_LOCALE` — локаль по умолчанию (например `en-US`)
- `DEFAULT_TIMEZONE_ID` — часовой пояс по умолчанию (например `UTC`)
- `DEFAULT_DATE_LOCALE` — локаль для парсинга дат по умолчанию (например `ru`)

### Настройки браузера
- `PLAYWRIGHT_HEADLESS` — режим без графического интерфейса (`true|false`)
- `PLAYWRIGHT_TIMEOUT_MS` — таймаут браузера в миллисекундах
- `PLAYWRIGHT_BLOCK_TRACKERS` — блокировать трекеры (`true|false`)
- `PLAYWRIGHT_BLOCK_HEAVY_RESOURCES` — блокировать тяжёлые ресурсы (`true|false`)
- `FINGERPRINT_GENERATE` — генерировать отпечаток автоматически (`true|false`)
- `FINGERPRINT_ROTATE_ON_ANTI_BOT` — вращать отпечаток только после антибот-повтора (`true|false`)

### Настройки пакетной обработки
- `BATCH_MIN_DELAY_MS` — минимальная задержка между запросами (по умолчанию `1500`)
- `BATCH_MAX_DELAY_MS` — максимальная задержка между запросами (по умолчанию `4000`)
- `BATCH_CONCURRENCY` — количество одновременных запросов (по умолчанию `1`)
- `BATCH_MAX_ITEMS` — максимальное количество элементов в пакете

### Настройки вебхуков
- `WEBHOOK_TIMEOUT_MS` — таймаут вебхука в миллисекундах (по умолчанию `10000`)
- `WEBHOOK_BACKOFF_MS` — начальная задержка между попытками повтора вебхука в миллисекундах (по умолчанию `1000`)
- `WEBHOOK_MAX_ATTEMPTS` — количество попыток повтора вебхука (по умолчанию `3`)


## Принцип работы микросервиса

### 1. Обработка запроса
1. Клиент отправляет POST-запрос на соответствующий эндпоинт
2. Сервис выполняет валидацию входных данных
3. Применяются настройки по умолчанию из переменных окружения
4. Запрос передаётся в модуль скрапинга

### 2. Процесс скрапинга
1. Выбор режима скрапинга (Cheerio или Playwright) в соответствии с запросом
2. Инициализация движка с указанными параметрами и генерация отпечатка браузера (если требуется)
3. Загрузка страницы с учётом настроек блокировки ресурсов и трекеров
4. Извлечение контента с использованием алгоритмов анализа страницы
5. Конвертация HTML-контента в формат Markdown
6. Формирование структурированного ответа

### 3. Пакетная обработка
1. Создание задачи в очереди с уникальным идентификатором
2. Последовательная или параллельная обработка элементов с задержками
3. Агрегация результатов обработки
4. Отправка уведомления на вебхук с результатами с учётом настроек повторных попыток

### 4. Работа с антибот защитой
1. Генерация уникального отпечатка браузера на основе настроенных ограничений
2. Применение настроек блокировки трекеров и тяжёлых ресурсов
3. При обнаружении антибот защиты - автоматическое вращение отпечатка (если включено)
4. Повторная попытка скрапинга с новым отпечатком браузера

## Типы данных

### Извлекаемые поля
- `title` — заголовок страницы/статьи
- `description` — описание или анонс
- `date` — дата публикации
- `author` — автор контента
- `body` — основное содержимое в формате Markdown
- `link` — URL страницы (для списков)
- `tags` — массив тегов или категорий

### Метаданные
- `lang` — язык страницы
- `readTimeMin` — примерное время чтения в минутах
- `total` — общее количество элементов
- `limit` — ограничение на количество элементов
- `scrapedAt` — время выполнения скрапинга

## Обработка ошибок

### Коды ошибок
- `400` — ошибка валидации входных данных
- `422` — не удалось извлечь контент со страницы
- `504` — таймаут загрузки страницы
- `502` — ошибка браузерного движка
- `500` — внутренняя ошибка сервера

### Формат ответа с ошибкой
```json
{
  "error": {
    "code": 422,
    "message": "Failed to extract content from the page",
    "details": "Page structure is not recognizable as an article"
  }
}
```

## Ограничения и безопасность

### Ограничения
- Ограничение на размер ответа
- Таймауты выполнения запросов
- Ограничение на количество одновременных сессий

## Мониторинг и логирование

### Логирование
- Структурированные логи в формате JSON с использованием pino
- Уровни логирования в соответствии с переменной окружения
- Контекстная информация для каждого запроса
- Логирование ключевых этапов обработки
